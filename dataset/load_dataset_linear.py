from .database_connection import collection
from .news_model import NewsObject
import torch
import re
import torchtext.data as data
from torchtext.vocab import Vectors, GloVe
from collections import Counter, OrderedDict

def extract_words(sentence):
    ignore = ['a', "the", "is"]
    words = re.sub("[^\w]", " ",  sentence).split()
    cleaned_text = [w.lower() for w in words if w not in ignore]
    for word in cleaned_text:
        if word.__contains__('--'):
            cleaned_text.remove('word')
    return cleaned_text

def get_vocabulary(exampleList):
    counter = Counter()
    for example in exampleList:
        counter.update(extract_words(example['content']))

    return counter.items()

def load():
    documents = list(collection.find())
    vocab_size = len(get_vocabulary(documents))

    # fixed length for the text field is mandatory when using logistic regreesion and the torchtext.vocab.Vocab generated by build_vocab has the first two positions
    #           reserved for "unknown key" and "padding key" keys so we pad the input at the beggining with 2 paddings and its length will be fixed to vocab_size + 2
    TEXT = data.Field(sequential=True, tokenize=extract_words, lower=True, include_lengths=True, batch_first=True, dtype=torch.float, fix_length=vocab_size + 2, pad_first=True)
    LABEL = data.LabelField(dtype=torch.float)

    examples = []
    for document in documents:
        example = data.Example.fromdict(document, fields={'content': ('content', TEXT), 'label': ('label', LABEL)})
        examples.append(example)
    dataset = data.Dataset(examples, [('content', TEXT), ('label', LABEL)])

    train_data, test_data = dataset.split(stratified=True, split_ratio=0.8)

    if embedding == 'glove':
        TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300, cache='./.linear_vector_cache'))
    elif embedding == 'fasttext':
        TEXT.build_vocab(train_data, vectors=FastText(language='en'))
    elif embedding == 'word2vec':
        word2vectors = Vectors('.word2vec_cache/GoogleNews-vectors-negative300.bin', cache='.word2vec_cache')
        TEXT.build_vocab(train_data, vectors=word2vectors)
    LABEL.build_vocab(dataset)

    word_embeddings = TEXT.vocab.vectors
    print ("Length of Text Vocabulary: " + str(len(list(set(TEXT.vocab.itos)))))
    print ("Vector size of Text Vocabulary: ", TEXT.vocab.vectors.size())
    print ("Label Length: " + str(len(LABEL.vocab)))

    train_data, valid_data = train_data.split(stratified=True, split_ratio=0.8) # Further splitting of training_data to create new training_data & validation_data
    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=4, sort_key=lambda x: len(x.content), repeat=False, shuffle=True)

    vocab_size = len(TEXT.vocab)

    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter